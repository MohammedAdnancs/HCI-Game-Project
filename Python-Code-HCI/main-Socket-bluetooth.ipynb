{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import asyncio\n",
    "from bleak import BleakScanner\n",
    "from datetime import datetime\n",
    "import asyncio\n",
    "import cv2 \n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from bluetooth import discover_devices, BluetoothSocket, RFCOMM\n",
    "from roboflow import Roboflow\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'DESKTOP-28SQ46K'  \n",
    "port = 8000        \n",
    "is_loggedin = False\n",
    "message_to_client = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to send \"pinching\" message to the server\n",
    "async def pinching_status_callback(pinching_detected,pinch_coordinates=None):\n",
    "    if pinching_detected:\n",
    "        if pinch_coordinates:\n",
    "            return(f\"Pinching:true,{pinch_coordinates[0]},{pinch_coordinates[1]}\")\n",
    "    # else:\n",
    "        # print(\"Not pinching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def getPointsRealTime(pinching_detected_callback,frame):\n",
    "\n",
    "    mp_holistic = mp.solutions.holistic\n",
    "    mp_hands = mp.solutions.hands\n",
    "    drawing_utils = mp.solutions.drawing_utils\n",
    "\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        \n",
    "            # Flip the frame horizontally to mirror the camera feed\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            \n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "\n",
    "            results = holistic.process(image)\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # Draw hand landmarks\n",
    "            drawing_utils.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "            drawing_utils.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "            # Check for pinch gesture\n",
    "            pinching_detected = False\n",
    "            pinch_coordinates = None\n",
    "            if results.right_hand_landmarks or results.left_hand_landmarks:\n",
    "                try:\n",
    "                    hand_landmarks = results.right_hand_landmarks or results.left_hand_landmarks\n",
    "                    index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "                    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "                    \n",
    "                    # Calculate distance between thumb and index fingertip to detect pinch\n",
    "                    distance = ((thumb_tip.x - index_tip.x)**2 + (thumb_tip.y - index_tip.y)**2) ** 0.5\n",
    "\n",
    "                    if distance < 0.08:  # Adjust threshold as needed\n",
    "                        pinching_detected = True\n",
    "                        # Calculate pinch coordinates\n",
    "                        pinch_x = int(index_tip.x * image.shape[1])\n",
    "                        pinch_y = int(index_tip.y * image.shape[0])\n",
    "                        pinch_coordinates = (pinch_x, pinch_y)\n",
    "                        # Draw a green dot at the pinch location\n",
    "                        cv2.circle(image, (pinch_x, pinch_y), 10, (0, 255, 0), -1)  # Draw green dot\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing hand landmarks: {e}\")\n",
    "\n",
    "            return await pinching_detected_callback(pinching_detected,pinch_coordinates)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def start_server(host, port, message_queue):\n",
    "    # Create a TCP/IP socket\n",
    "    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    \n",
    "    # Bind the socket to the address and port\n",
    "    server_socket.bind((host, port))\n",
    "    \n",
    "    # Enable the server to listen for incoming connections\n",
    "    server_socket.listen(5)\n",
    "    print(f\"Server listening on {host}:{port}...\")\n",
    "    \n",
    "    # Wait for a client to connect\n",
    "    client_socket, client_address = await asyncio.to_thread(server_socket.accept)\n",
    "    print(f\"Client connected: {client_address}\")\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            message_to_client = await message_queue.get()  # Wait for a message from the queue\n",
    "            if message_to_client:\n",
    "                client_socket.sendall(message_to_client.encode())\n",
    "            message_queue.task_done()  # Mark the message as processed\n",
    "    finally:\n",
    "        # Clean up the connection\n",
    "        client_socket.close()\n",
    "        server_socket.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_sockets = {}\n",
    "# Initialize MediaPipe Hands module\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def track_hand_and_get_index_finger(frame):\n",
    "    \n",
    "\n",
    "    # Convert the image color to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame and detect hands\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    index_finger_coords = None\n",
    "    name = \"No hand detected\"\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Get the x and y coordinates of the index finger tip (landmark 8)\n",
    "            index_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "\n",
    "            # Convert the normalized coordinates to pixel values\n",
    "            h, w, c = frame.shape\n",
    "            index_x = int(index_finger_tip.x * w)\n",
    "            index_y = int(index_finger_tip.y * h)\n",
    "\n",
    "            # Draw a circle at the index finger tip\n",
    "            cv2.circle(frame, (index_x, index_y), 10, (255, 0, 0), -1)\n",
    "\n",
    "            # Determine if it's a left or right hand based on landmarks\n",
    "            if hand_landmarks.landmark[mp_hands.HandLandmark.WRIST].x < hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_MCP].x:\n",
    "                name = \"Left hand\"\n",
    "            else:\n",
    "                name = \"Right hand\"\n",
    "                \n",
    "            index_finger_coords = (index_x, index_y)\n",
    "\n",
    "    return name, index_finger_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def initialize_emotion_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "    \n",
    "    # Load the trained weights\n",
    "    await asyncio.to_thread(model.load_weights, 'model.h5')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def predict_emotion(frame, model, emotion_dict):\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Initialize the face cascade for face detection\n",
    "    facecasc = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # Detect faces in the image\n",
    "    faces = facecasc.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    # Process the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw a rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        \n",
    "        # Extract the region of interest (ROI) for emotion prediction\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n",
    "\n",
    "        # Suppress TensorFlow/Keras verbosity\n",
    "        tf.get_logger().setLevel('ERROR')  # Suppress verbose logging\n",
    "\n",
    "         # If model is async, we need to await it\n",
    "        if asyncio.iscoroutinefunction(model.predict):\n",
    "            prediction = await model.predict(cropped_img, verbose=0)\n",
    "        else:\n",
    "            # If it's not async, use asyncio.to_thread to run it in a separate thread\n",
    "            prediction = await asyncio.to_thread(model.predict, cropped_img, verbose=0)\n",
    "        \n",
    "        maxindex = int(np.argmax(prediction))\n",
    "\n",
    "        # Return the predicted emotion label\n",
    "        return emotion_dict[maxindex]\n",
    "    \n",
    "    return None  # If no face is detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def face_recognition(frame, recognizer, faceCascade, names):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = faceCascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        return \"No Face\", None  # Return default values if no faces are detected\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw a rectangle around the detected face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Recognize the face using the trained model\n",
    "        id, confidence = recognizer.predict(gray[y:y + h, x:x + w])\n",
    "\n",
    "        if confidence > 10:\n",
    "            name = names[id]\n",
    "        else:\n",
    "            name = \"Unknown\"\n",
    "\n",
    "        # Return the name and the coordinates of the face\n",
    "        return name, (x, y, w, h)\n",
    "    \n",
    "    return \"No Face\", None  # In case the loop ends without detecting a face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized: No Face, Emotion: Unknown,Pinching:None,Index Finger coordinates: ('No hand detected', None)\n",
      "Recognized: MohammedAdnan, Emotion: None,Pinching:None,Index Finger coordinates: ('No hand detected', None)\n",
      "Recognized: No Face, Emotion: Unknown,Pinching:None,Index Finger coordinates: ('No hand detected', None)\n",
      "Recognized: No Face, Emotion: Unknown,Pinching:None,Index Finger coordinates: ('No hand detected', None)\n",
      "Recognized: MohammedAdnan, Emotion: None,Pinching:None,Index Finger coordinates: ('No hand detected', None)\n",
      "Recognized: No Face, Emotion: Unknown,Pinching:None,Index Finger coordinates: ('No hand detected', None)\n",
      "Recognized: No Face, Emotion: Unknown,Pinching:None,Index Finger coordinates: ('No hand detected', None)\n",
      "Recognized: MohammedAdnan, Emotion: None,Pinching:None,Index Finger coordinates: ('No hand detected', None)\n",
      "Recognized: MohammedAdnan, Emotion: None,Pinching:None,Index Finger coordinates: ('Left hand', (1356, 376))\n",
      "Recognized: MohammedAdnan, Emotion: Sad,Pinching:None,Index Finger coordinates: ('Right hand', (1243, 423))\n",
      "Recognized: MohammedAdnan, Emotion: None,Pinching:None,Index Finger coordinates: ('No hand detected', None)\n",
      "Recognized: MohammedAdnan, Emotion: None,Pinching:None,Index Finger coordinates: ('No hand detected', None)\n",
      "Recognized: MohammedAdnan, Emotion: Sad,Pinching:None,Index Finger coordinates: ('No hand detected', None)\n",
      "Recognized: MohammedAdnan, Emotion: None,Pinching:None,Index Finger coordinates: ('No hand detected', None)\n",
      "Recognized: MohammedAdnan, Emotion: None,Pinching:None,Index Finger coordinates: ('Left hand', (1387, 636))\n",
      "Recognized: MohammedAdnan, Emotion: None,Pinching:Pinching:true,613,604,Index Finger coordinates: ('Right hand', (1285, 619))\n",
      "Recognized: MohammedAdnan, Emotion: None,Pinching:Pinching:true,619,596,Index Finger coordinates: ('Right hand', (1278, 608))\n",
      "Recognized: MohammedAdnan, Emotion: Fearful,Pinching:Pinching:true,634,580,Index Finger coordinates: ('Right hand', (1264, 600))\n",
      "Recognized: MohammedAdnan, Emotion: Sad,Pinching:Pinching:true,659,581,Index Finger coordinates: ('Left hand', (1245, 580))\n",
      "Recognized: MohammedAdnan, Emotion: None,Pinching:Pinching:true,691,588,Index Finger coordinates: ('Right hand', (1214, 584))\n",
      "Recognized: MohammedAdnan, Emotion: None,Pinching:Pinching:true,719,622,Index Finger coordinates: ('Right hand', (1176, 620))\n",
      "Recognized: MohammedAdnan, Emotion: None,Pinching:Pinching:true,762,614,Index Finger coordinates: ('Right hand', (1141, 624))\n",
      "Recognized: MohammedAdnan, Emotion: None,Pinching:Pinching:true,804,627,Index Finger coordinates: ('Right hand', (1111, 636))\n",
      "Recognized: MohammedAdnan, Emotion: None,Pinching:Pinching:true,803,594,Index Finger coordinates: ('Right hand', (1093, 583))\n",
      "Recognized: MohammedAdnan, Emotion: None,Pinching:Pinching:true,775,546,Index Finger coordinates: ('Right hand', (1121, 543))\n",
      "Recognized: MohammedAdnan, Emotion: None,Pinching:None,Index Finger coordinates: ('Right hand', (1194, 160))\n",
      "Recognized: MohammedAdnan, Emotion: None,Pinching:None,Index Finger coordinates: ('Right hand', (1112, 445))\n",
      "Recognized: MohammedAdnan, Emotion: None,Pinching:Pinching:true,793,523,Index Finger coordinates: ('Right hand', (1115, 517))\n",
      "Recognized: MohammedAdnan, Emotion: None,Pinching:Pinching:true,803,525,Index Finger coordinates: ('Right hand', (1100, 508))\n",
      "Recognized: MohammedAdnan, Emotion: Happy,Pinching:None,Index Finger coordinates: ('Right hand', (1185, 129))\n",
      "Recognized: MohammedAdnan, Emotion: None,Pinching:None,Index Finger coordinates: ('Right hand', (1088, 448))\n",
      "Recognized: MohammedAdnan, Emotion: Fearful,Pinching:Pinching:true,805,533,Index Finger coordinates: ('Right hand', (1120, 511))\n",
      "Recognized: MohammedAdnan, Emotion: None,Pinching:Pinching:true,806,508,Index Finger coordinates: ('Right hand', (1112, 512))\n",
      "Recognized: MohammedAdnan, Emotion: None,Pinching:None,Index Finger coordinates: ('Right hand', (1144, 414))\n",
      "Recognized: MohammedAdnan, Emotion: Surprised,Pinching:None,Index Finger coordinates: ('No hand detected', None)\n",
      "Recognized: MohammedAdnan, Emotion: Happy,Pinching:None,Index Finger coordinates: ('No hand detected', None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-670' coro=<start_server() done, defined at C:\\Users\\medoa\\AppData\\Local\\Temp\\ipykernel_20804\\229596483.py:1> exception=OSError(10048, 'Only one usage of each socket address (protocol/network address/port) is normally permitted', None, 10048, None)>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\medoa\\AppData\\Local\\Temp\\ipykernel_20804\\229596483.py\", line 6, in start_server\n",
      "    server_socket.bind((host, port))\n",
      "OSError: [WinError 10048] Only one usage of each socket address (protocol/network address/port) is normally permitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [INFO] Exiting Program.\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    message_queue = asyncio.Queue()  # Create a queue to send messages to the server\n",
    "    socket_task = asyncio.create_task(start_server(host, port,message_queue))\n",
    "    # Create LBPH Face Recognizer\n",
    "    recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "    # Load the trained model\n",
    "    recognizer.read('trainer.yml')\n",
    "\n",
    "    # Path to the Haar cascade file for face detection\n",
    "    face_cascade_Path = \"haarcascade_frontalface_default.xml\"\n",
    "    # Create a face cascade classifier\n",
    "    faceCascade = cv2.CascadeClassifier(face_cascade_Path)\n",
    "\n",
    "    # Initialize user IDs and associated names\n",
    "    names = ['None']\n",
    "    is_pinching=None\n",
    "    with open('names.json', 'r') as fs:\n",
    "        names = json.load(fs)\n",
    "        names = list(names.values())\n",
    "    \n",
    "    # Initialize emotion model\n",
    "    emotion_model = await initialize_emotion_model()\n",
    "    \n",
    "    # Define emotion dictionary\n",
    "    emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "\n",
    "    # Video Capture from the default camera (camera index 0)\n",
    "    cam = cv2.VideoCapture(0)\n",
    "    cam.set(3, 640)  # Set width\n",
    "    cam.set(4, 480)  # Set height\n",
    "\n",
    "    # Create the window\n",
    "    cv2.namedWindow('camera', cv2.WINDOW_NORMAL)\n",
    "    # Resize the window\n",
    "    cv2.resizeWindow('camera', 1700, 980)  # Change the values as needed (e.g., 320, 240 for a smaller window)\n",
    "\n",
    "    last_sent_time = time.time()  # Initialize the timer\n",
    "\n",
    "    while True:\n",
    "        # Read a frame from the camera\n",
    "        ret, frame = cam.read()\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        if not ret:\n",
    "            print(\"[ERROR] Failed to read from the camera.\")\n",
    "            break\n",
    "\n",
    "        # Call face recognition function\n",
    "        name, face_coords = await asyncio.create_task(face_recognition(frame, recognizer, faceCascade, names))\n",
    "        emotion = \"Unknown\"\n",
    "        index_finger_coords = \"Unknown\"\n",
    "        index_finger_coords = await asyncio.create_task(track_hand_and_get_index_finger(frame))\n",
    "        is_pinching = await asyncio.create_task(getPointsRealTime(pinching_status_callback,frame))\n",
    "        # Display the frame with landmarks and index finger tip\n",
    "        if index_finger_coords and isinstance(index_finger_coords, tuple) and len(index_finger_coords) == 2:\n",
    "            x, y = index_finger_coords\n",
    "            # Ensure that x and y are integers\n",
    "            if isinstance(x, int) and isinstance(y, int):\n",
    "                cv2.circle(frame, (x, y), 10, (128, 0, 128), 3)  # Draw the circle\n",
    "       \n",
    "        if face_coords:\n",
    "            # Extract the face region for emotion prediction\n",
    "            x, y, w, h = face_coords\n",
    "            face_region = frame[y:y + h, x:x + w]\n",
    "\n",
    "            # Call emotion prediction function\n",
    "            emotion = await predict_emotion(face_region, emotion_model, emotion_dict)\n",
    "\n",
    "            # Display the recognized emotion\n",
    "            cv2.putText(frame, emotion, (x+20, y-40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "       \n",
    "        cv2.putText(frame, name, (5, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (128, 0, 128), 2)\n",
    "        cv2.putText(frame, emotion, (5, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (128, 0, 128), 2)\n",
    "        cv2.putText(frame, str(index_finger_coords), (5, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (128, 0, 128), 2)\n",
    "        cv2.putText(frame, str(is_pinching), (5, 210), cv2.FONT_HERSHEY_SIMPLEX, 1, (128, 0, 128), 2)\n",
    "        # Check if 5 seconds have passed since the last recognition\n",
    "        \n",
    "        current_time = time.time()\n",
    "        if current_time - last_sent_time >= 0.5:\n",
    "            print(f\"Recognized: {name}, Emotion: {emotion},Pinching:{is_pinching},Index Finger coordinates: {index_finger_coords}\")\n",
    "            last_sent_time = current_time  # Update the timer\n",
    "         # Send the message to the server via the queue\n",
    "\n",
    "        message_to_client = f\" Recognized: {name} , Emotion: {emotion} , Pinching:{is_pinching} , Index Finger coordinates: {index_finger_coords}\"\n",
    "        await message_queue.put(message_to_client)  # Put the message in the queue\n",
    "\n",
    "        # Display the image with rectangles around faces and objects\n",
    "        cv2.imshow('camera', frame)\n",
    "        # Press Escape or 'q' to exit the webcam/program\n",
    "        k = cv2.waitKey(10) & 0xff\n",
    "        if k == 27 or k == ord('q'):  # 27 is the Escape key; ord('q') checks for 'q'\n",
    "            break\n",
    "\n",
    "    print(\"\\n [INFO] Exiting Program.\")\n",
    "    # Release the camera\n",
    "    cam.release()\n",
    "    # Close all OpenCV windows\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
